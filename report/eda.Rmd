---
output: html_document
editor_options: 
  chunk_output_type: console
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, message=FALSE, cache=FALSE, warning=FALSE)
source(here::here("script/setup.R"))
```

## **Data Preparation**

In this section, we will explain the steps we did for the retrieval of the data from the US Amazon marketplace. In Addition, we will explain some of the methods we had to apply to the text reviews in order to have the final tables. It is worth mentioning that due to the heavy computation on this section we will present the final outcomes and the examples (before and after) of the wrangling and cleaning. The code can be observe on the wrangling.RMD file.

### **Data Retrieval**

During this section, we have scrapped from the US Amazon marketplace the smartphone reviews of Apple and Samsung phones. We have decided to retrieve the reviews of the eleventh, twelfth, and the thirteenth generations of Apple. Also, we have consider the Generation S20, 2S21, and S22 for Samsung. The phones concerning each of the generation with the amount of reviews are described as follows:

* **Apple - 12,965 obs**
    + Generation 11th - **10,276 obs**
        * iPhone 11 - 5,414 obs
        * iPhone 11 Pro - 2,844 obs
        * iPhone 11 Pro Max - 2,414 obs
    + Generation 12th - **2,239 obs**
        * iPhone 12 - 975 obs
        * iPhone 12 Mini - 779 obs
        * iPhone 12 Pro - 376 obs
        * iPhone 12 Pro Max - 109 obs
    + Generation 13th - **450 obs**
        * iPhone 13 - 152 obs
        * iPhone 13 Mini - 146 obs
        * iPhone 13 Pro - 68 obs
        * iPhone 13 Pro Max - 84 obs

* **Samsung - 2,606 obs**
    + Generation S20 - **177 obs**
        * Samsung Galaxy S20 FE - 63 obs
        * Samsung Galaxy S20 Plus - 85 obs
        * Samsung Galaxy Note S20 Ultra - 29 obs
    + Generation S21 - **2,166 obs**
        * Samsung Galaxy S21 FE - 1,775 obs
        * Samsung Galaxy S21 Plus - 361 obs
        * Samsung Galaxy S21 Ultra - 30 obs
    + Generation S22 - **263 obs**
        * Samsung Galaxy S22 - 47 obs
        * Samsung Galaxy S22 Plus - 62 obs
        * Samsung Galaxy S22 Ultra - 154 obs
        
From this numbers we can say that Apple mobile phones are more popular in the US than Samsung phones, as we observe that users are more willingly to review the phones from this brand. Furthermore, the Generation 11 of Apple had the largest amount of reviews compared to the rest with the iPhone 11 at the top of the list. 


```{r}
smartphone_reviews <- read.csv(here::here("data/smartphone_reviews.csv"))

# Plot Table
smartphone_reviews[12983:12992,] %>%
  kable(caption = "Amazon US smartphones reviews - Apple and Samsung") %>%
  kable_styling(bootstrap_options = c("striped","hover","condensed"),
                fixed_thead = T) %>% scroll_box(width = "100%", height = "300px")
```

### **Reviews Language Detection** (*Text Classification*)

After scrapping the reviews from the smartphones we noticed that some of them were written in languages different from English. For that reason, we decided to use Transformers from the Hugging Face ðŸ¤— website throughout the `pipelines()` function in order to apply tasks such as **text classification and text translations**. First, we applied the classification task from the model [eleldar/language-detection](https://huggingface.co/eleldar/language-detection), which is a fine-tuned version of [xlm-roberta-base](https://huggingface.co/xlm-roberta-base) on the Language Identification dataset. By using this model we were able to detect the language of the reviews from our dataset.

After the application of the task `text-classification` and the model to the dataset, we created a column called *Language* to determine the language and the amount of reviews of each of them.

```{r}
apple_class <- read.csv(here::here("data/apple_class.csv"))
samsung_class <- read.csv(here::here("data/samsung_class.csv"))

# Plot Table
apple_class[1239:1244,] %>%
  kable(caption = "Appe reviews - Language Detection") %>%
  kable_styling(bootstrap_options = c("striped","hover","condensed"),
                fixed_thead = T) %>% scroll_box(width = "100%", height = "250px")
samsung_class[51:57,] %>%
  kable(caption = "Samsung reviews - Language Detection") %>%
  kable_styling(bootstrap_options = c("striped","hover","condensed"),
                fixed_thead = T) %>% scroll_box(width = "100%", height = "250px")
```

### **Reviews Language Translation**

For this part, we used the task for `text translations` from the model [Helsinki-NLP/opus-mt-es-en](https://huggingface.co/Helsinki-NLP/opus-mt-es-en?text=Me+llamo+Wolfgang+y+vivo+en+Berlin), which helped us to translate the reviews written in Spanish to English. We have only considered to translate this language because it was the second most representative language in our data. Languages like French, Japanese, and Hindi that we identified in our data had less than 10 observations.

For visualization purposes, we have combined both tables (input and output), to show how the translation was executed. This means that we kept only the reviews translated to English for our Analysis.

```{r}
apple_es <- read.csv(here::here("data/apple_es.csv"))
apple_es_en <- read.csv(here::here("data/apple_es_en.csv"))
samsung_es <- read.csv(here::here("data/samsung_es.csv"))
samsung_es_en <- read.csv(here::here("data/samsung_es_en.csv"))

# Apple
l <- list(a=apple_es,b=apple_es_en)
both_apple <- do.call(rbind, l)[order(sequence(sapply(l, nrow))), ]
both_apple[19:24,] %>%
  kable(caption = "Apple reviews - Language Translation") %>%
  kable_styling(bootstrap_options = c("striped","hover","condensed"),
                fixed_thead = T) %>% scroll_box(width = "100%", height = "250px")

# Samsung
s <- list(a=samsung_es,b=samsung_es_en)
both_samsung <- do.call(rbind, s)[order(sequence(sapply(s, nrow))), ]
both_samsung[1:6,] %>%
  kable(caption = "Samsung reviews - Language Translation") %>%
  kable_styling(bootstrap_options = c("striped","hover","condensed"),
                fixed_thead = T) %>% scroll_box(width = "100%", height = "250px")
```


## **Exploratory Data Analysis**

### Tokenization and Cleaning

Now that we have all our text in English solely, we can start processing our dataset to transform it into a corpus. The second step is to create tokens - each tokens will be assigned a word - and to get rid of non conforming formats. Thereby, we are removing in our corpus any punctuation; symbols; numbers; separator. To have a better analysis we also decided to remove "stop words" which correspond to parasite words - in other terms, words such as "a" "the" etc... that do not add value to the analysis. Finally instead of using a steeming method, we decided to proceed with a lemmatization technique - this correspond to the usage of a lexicon dictionary that will look for the root of words, in order to get rid of unuseful repetition with minor change - teach / teaching / taught will all be reduced to the root teach. Prio to continue with graphical represenation, we also compute the following informations:

*DTM - Document Term matrix corresponds to the number of time a specif terms will appear
*TF-IDF -  Term Frequency Inverse Document frequency is a measure to quantify the relevance of a particular words in a document. 
*Global Frequency corresponds to the frequency of each words for each document by rank.

```{r}
Smartphone_reviews <- read.csv(here::here("data/smartphone_reviews_final.csv"))

#Creating a corpus
Smartphone_reviews.cp <- corpus(Smartphone_reviews$Reviews)

#Creating tokens
Smartphone_reviews.tk <- tokens(
  Smartphone_reviews.cp,
  remove_punct = TRUE,
  remove_symbols = TRUE,
  remove_numbers = TRUE,
  remove_separators = TRUE
)
Smartphone_reviews.tk <- Smartphone_reviews.tk %>%
  tokens_tolower() %>%
  tokens_remove(stop_words$word)

#lemmatization of words

Smartphone_reviews.tk <- Smartphone_reviews.tk %>%  
  tokens_replace(
  pattern = hash_lemmas$token,
  replacement = hash_lemmas$lemma)

## Compute the DTM, TF-IDF and global frequencies
Smartphone_reviews.dfm <- dfm(Smartphone_reviews.tk)
Smartphone_reviews.tfidf <- dfm_tfidf(Smartphone_reviews.dfm)  
Smartphone_reviews.freq <- textstat_frequency(Smartphone_reviews.dfm)
```

#### Plotting frequency

```{r}
Smartphone_reviews.freq %>% 
  top_n(20, frequency) %>%
  ggplot(aes(
    x = reorder(feature, frequency),
    y = frequency)) + 
  geom_bar(stat = "identity") + 
  coord_flip() +
  xlab("Frequency") + 
  ylab("term")
```

#### For document

```{r}
Smartphone_reviews.dfm %>% 
  tidy() %>% 
  group_by(document) %>% 
  top_n(5, count) %>% 
  ungroup() %>% 
  ggplot(aes(x = term, y = count)) + 
  geom_col() + coord_flip() + 
  theme(axis.text.y = element_text(size = 4),
        axis.ticks.y = element_blank())  + 
  facet_wrap(~document, ncol = 2, scales="free")
```

#### Plotting TF-IDF

```{r}
Smartphone_reviews.tfidf %>% 
  tidy() %>% 
  top_n(20, count) %>% 
  ggplot(aes(x = term, y = count)) + 
  geom_col() + 
  coord_flip() + 
  theme(axis.text.y = element_text(size = 4),
        axis.ticks.y = element_blank())  + 
  facet_wrap(~document, ncol = 2)
```

#### Maximum TF_IDF per document

```{r}
Smartphone_reviews.tfidf %>% 
  tidy() %>%
  group_by(term) %>%
  summarize(count = max(count)) %>%
  ungroup() %>% 
  arrange(desc(count)) %>%
  top_n(20, count) %>%
  ggplot(aes(x=reorder(term, count),
             y = count)) + 
  geom_bar(stat = "identity") + 
  coord_flip() +
  xlab("Max TF-IDF") + 
  ylab("term")
```

#### Document log frequency

```{r}
Smartphone_reviews.freq %>% 
  ggplot(aes(x = log10(docfreq),
             y = log10(frequency))) + 
  geom_text(aes(label=feature),
            position=position_jitter(),
            size = 3) + 
  xlab("Document log-frequency") + 
  ylab("log-frequency")
```
