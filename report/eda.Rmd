---
output: html_document
editor_options: 
  chunk_output_type: console
---

```{r, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, message=FALSE, cache=FALSE, warning=FALSE)
source(here::here("script/setup.R"))
```

## **Data Preparation**

In this section, we will explain the steps we did for the retrieval of the data from the US Amazon marketplace. In Addition, we will explain some of the tasks we had to apply to the text reviews in order to have the final tables. It is worth mentioning that due to the heavy computation on this section we will present the final outcomes and the examples (before and after) of the wrangling and cleaning. The code can be observed on the scrapping.RMD file.

### **Data Retrieval**

During this section, we have scrapped from the US Amazon marketplace the smartphone reviews of Apple and Samsung phones. We have decided to retrieve the reviews of the eleventh, twelfth, and the thirteenth generations of Apple. Also, we have consider the Generation S20, 2S21, and S22 for Samsung. The phones concerning each of the generation with the amount of reviews are described as follows:

* **Apple - 12,965 obs**
    + Generation 11th - **10,276 obs**
        * iPhone 11 - 5,414 obs
        * iPhone 11 Pro - 2,844 obs
        * iPhone 11 Pro Max - 2,414 obs
    + Generation 12th - **2,239 obs**
        * iPhone 12 - 975 obs
        * iPhone 12 Mini - 779 obs
        * iPhone 12 Pro - 376 obs
        * iPhone 12 Pro Max - 109 obs
    + Generation 13th - **450 obs**
        * iPhone 13 - 152 obs
        * iPhone 13 Mini - 146 obs
        * iPhone 13 Pro - 68 obs
        * iPhone 13 Pro Max - 84 obs

* **Samsung - 2,606 obs**
    + Generation S20 - **177 obs**
        * Samsung Galaxy S20 FE - 63 obs
        * Samsung Galaxy S20 Plus - 85 obs
        * Samsung Galaxy Note S20 Ultra - 29 obs
    + Generation S21 - **2,166 obs**
        * Samsung Galaxy S21 FE - 1,775 obs
        * Samsung Galaxy S21 Plus - 361 obs
        * Samsung Galaxy S21 Ultra - 30 obs
    + Generation S22 - **263 obs**
        * Samsung Galaxy S22 - 47 obs
        * Samsung Galaxy S22 Plus - 62 obs
        * Samsung Galaxy S22 Ultra - 154 obs
        
From this numbers we can say that Apple mobile phones are more popular in the US than Samsung phones, as we observe that users are more willingly to review the phones from this brand. Furthermore, the Generation 11 of Apple had the largest amount of reviews compared to the rest with the iPhone 11 at the top of the list. 


```{r}
smartphone_reviews <- read.csv(here::here("data/smartphone_reviews.csv"))

# Plot Table
smartphone_reviews[12983:12992,] %>%
  kable(caption = "Amazon US smartphones reviews - Apple and Samsung") %>%
  kable_styling(bootstrap_options = c("striped","hover","condensed"),
                fixed_thead = T) %>% scroll_box(width = "100%", height = "300px")
```

### **Reviews Language Detection** (*Text Classification*)

After scrapping the reviews from the smartphones we noticed that some of them were written in languages different from English. For that reason, we decided to use Transformers from the Hugging Face ðŸ¤— website throughout the `pipelines()` function in order to apply tasks such as **text classification and text translations**. First, we applied the classification task from the model [eleldar/language-detection](https://huggingface.co/eleldar/language-detection), which is a fine-tuned version of [xlm-roberta-base](https://huggingface.co/xlm-roberta-base) on the Language Identification dataset. By using this model we were able to detect the language of the reviews from our dataset.

After the application of the task `text-classification` and the model to the dataset, we created a column called *Language* to determine the language and the amount of reviews of each of them.

```{r}
apple_class <- read.csv(here::here("data/apple_class.csv"))
samsung_class <- read.csv(here::here("data/samsung_class.csv"))

# Plot Table
apple_class[1239:1244,] %>%
  kable(caption = "Appe reviews - Language Detection") %>%
  kable_styling(bootstrap_options = c("striped","hover","condensed"),
                fixed_thead = T) %>% scroll_box(width = "100%", height = "250px")
samsung_class[51:57,] %>%
  kable(caption = "Samsung reviews - Language Detection") %>%
  kable_styling(bootstrap_options = c("striped","hover","condensed"),
                fixed_thead = T) %>% scroll_box(width = "100%", height = "250px")
```

### **Reviews Language Translation**

For this part, we used the task for `text translations` from the model [Helsinki-NLP/opus-mt-es-en](https://huggingface.co/Helsinki-NLP/opus-mt-es-en?text=Me+llamo+Wolfgang+y+vivo+en+Berlin), which helped us to translate the reviews written in Spanish to English. We have only considered to translate this language because it was the second most representative language in our data. Languages like French, Japanese, and Hindi that we identified in our data had less than 10 observations.

For visualization purposes, we have combined both tables (input and output), to show how the translation was executed. This means that we kept only the reviews translated to English for our Analysis.

```{r}
apple_es <- read.csv(here::here("data/apple_es.csv"))
apple_es_en <- read.csv(here::here("data/apple_es_en.csv"))
samsung_es <- read.csv(here::here("data/samsung_es.csv"))
samsung_es_en <- read.csv(here::here("data/samsung_es_en.csv"))

# Apple
l <- list(a=apple_es,b=apple_es_en)
both_apple <- do.call(rbind, l)[order(sequence(sapply(l, nrow))), ]
both_apple[19:24,] %>%
  kable(caption = "Apple reviews - Language Translation") %>%
  kable_styling(bootstrap_options = c("striped","hover","condensed"),
                fixed_thead = T) %>% scroll_box(width = "100%", height = "250px")

# Samsung
s <- list(a=samsung_es,b=samsung_es_en)
both_samsung <- do.call(rbind, s)[order(sequence(sapply(s, nrow))), ]
both_samsung[1:6,] %>%
  kable(caption = "Samsung reviews - Language Translation") %>%
  kable_styling(bootstrap_options = c("striped","hover","condensed"),
                fixed_thead = T) %>% scroll_box(width = "100%", height = "250px")
```

After dealing with the steps explained above, we have transformed our files from having reviews with different languages to a final file with reviews in English. Thus, the number of reviews has slightly decreased, in the case of Apple from **12,965 obs to 12,712 obs**, and in the case of Samsung from **2,606 obs to 2,585**.

## **Exploratory Data Analysis**

### Tokenization and Cleaning

Now that we have all our text in English solely, we can start processing our dataset to transform it into a corpus. The second step is to create tokens - each tokens will be assigned a word - and to get rid of non conforming formats. Thereby, we are removing in our corpus any punctuation; symbols; numbers; separator. To have a better analysis we also decided to remove "stop words" which correspond to parasite words - in other terms, words such as "a" "the" etc... that do not add value to the analysis. Finally instead of using a steeming method, we decided to proceed with a lemmatization technique - this correspond to the usage of a lexicon dictionary that will look for the root of words, in order to get rid of unuseful repetition with minor change - teach / teaching / taught will all be reduced to the root teach. Prior to continue with graphical representation, we also compute the following information:

*DTM - Document Term matrix corresponds to the number of time a specif terms will appear
*TF-IDF -  Term Frequency Inverse Document frequency is a measure to quantify the relevance of a particular words in a document. 
*Global Frequency corresponds to the frequency of each words for each document by rank.

```{r}
Smartphone_reviews <- read.csv(here::here("data/smartphone_reviews_final.csv"))
row.name <- as.vector(Smartphone_reviews$Model)
row.name <- paste(row.name, "_", sep = "")
row.name <- paste0(row.name, 1:15297)
row.names(Smartphone_reviews) <- row.name

#Creating a corpus
Smartphone_reviews.cp <- corpus(Smartphone_reviews, text_field = "Reviews",
               meta = list(source = "Smartphone reviews"))
summary(Smartphone_reviews.cp) %>% head(10)

# Creating Corpus by Group Model
Smartphone_reviews.cp.group <- corpus_group(Smartphone_reviews.cp, groups = Model)
summary(Smartphone_reviews.cp.group) 

#Creating tokens
Smartphone_reviews.tk <- tokens(
  Smartphone_reviews.cp,
  remove_punct = TRUE,
  remove_symbols = TRUE,
  remove_numbers = TRUE,
  remove_separators = TRUE) %>%
  tokens_tolower() %>%
  tokens_remove(stop_words$word)

#lemmatization of words

Smartphone_reviews.tk <- Smartphone_reviews.tk %>%  
  tokens_replace(
  pattern = hash_lemmas$token,
  replacement = hash_lemmas$lemma)

## Compute the DTM, TF-IDF and global frequencies
Smartphone_reviews.dfm <- dfm(Smartphone_reviews.tk)
Smartphone_reviews.tfidf <- dfm_tfidf(Smartphone_reviews.dfm)  
Smartphone_reviews.freq <- textstat_frequency(Smartphone_reviews.dfm)

## Compute the DTM, TF-IDF and global frequencies per group
Smartphone_reviews.dfm.group <- dfm_group(Smartphone_reviews.dfm, groups = Model)
Smartphone_reviews.tfidf.group <- dfm_tfidf(Smartphone_reviews.dfm.group)  
Smartphone_reviews.freq.group <- textstat_frequency(Smartphone_reviews.dfm.group)

```

#### Plotting frequency

Observing the frequency plot we can analyse that the most common word used was "phone" followed by "battery" and "screen". While phone comes without surprise as  we got our information from phones reviews on amazon, an interesting point came from the two most used words afterwards. Indeed, we might affirm, with this graphs that the hottest topic for a consumers is the battery life of a new phone and the quality of its screen rather than the software behind it nor the features added. 

```{r}
Smartphone_reviews.freq %>% 
  top_n(20, frequency) %>%
  ggplot(aes(
    x = reorder(feature, frequency),
    y = frequency)) + 
  geom_bar(stat = "identity") + 
  coord_flip() +
  xlab("Frequency") + 
  ylab("term")
```

#### For document

This graph focused on the frequency of the 5 more common words used for each of the documents available. An interesting thing, to note is that in our dataset more than 80% of the reviews came from apple buyers. Nonetheless, in this sample showed, it appears that most words used are related to android instead of IOS. Another interesting fact to note is that the most common word used all through out the corpus are - samsung, s9, s20, phone, onplus, entry, device, camera, apple.

```{r}
Smartphone_reviews.dfm %>% 
  tidy() %>% 
  top_n(10, count) %>% 
  ggplot(aes(x = term, y = count)) + 
  geom_bar(stat = "Identity") + 
  coord_flip() +
  theme(axis.text.y = element_text(size = 8),
        axis.ticks.y = element_blank())  + 
  facet_wrap(~document, ncol = 2)

# Plot per document grouped 
Smartphone_reviews.tfidf.group %>% 
  tidy() %>% 
  top_n(10, count) %>% 
  ggplot(aes(x = term, y = count)) + 
  geom_bar(stat = "Identity") + 
  coord_flip() +
  theme(axis.text.y = element_text(size = 8),
        axis.ticks.y = element_blank())  + 
  facet_wrap(~document, ncol = 2)

```

#### Plotting maximum TF_IDF per term and document

Due to the large amount of reviews, we decided to create a visualization of the max TF-IDF for each documents instead of showing each one of them. Thereby, the following representation tell us that each of those words have at least a large TF-IDF in on document. Analyzing the output, entry seems to be the word having the most relevance followed then by "oneplus" "s9" "s20". While the words ranked 2 to 4 are all specif to model, it turns out that "entry" is what matter most for a consumers. Entry in the context of purchase might corresponds to leader price. 

```{r, fig.width=10}
#Plotting maximum TF_IDF per term
Smartphone_reviews.tfidf %>% 
  tidy() %>%
  group_by(term) %>%
  summarize(count = max(count)) %>%
  ungroup() %>% 
  arrange(desc(count)) %>%
  top_n(20, count) %>%
  ggplot(aes(x=reorder(term, count),
             y = count)) + 
  geom_bar(stat = "identity") + 
  coord_flip() +
  xlab("Max TF-IDF") + 
  ylab("term")


# Plotting maximum TF_IDF per document grouped

Smartphone_reviews.tfidf.group %>% 
  tidy() %>%
  group_by(document) %>%
  slice_max(count, n = 5) %>%
  ungroup() %>%
  ggplot(aes(count, reorder_within(term, count, document), fill = document)) +
  geom_col(show.legend = FALSE) + scale_y_reordered() +
  facet_wrap(~document, ncol = 4, scales = "free") +
  labs(x = "tf-idf", y = NULL)
```


#### Document log frequency

While this log frequency representation is quite messy due to the amount of document used, we can still distinguish the tokens found during the frequency plot representation. Indeed - phone, battery, screen - are the most common words used and are present in nearly every document. On the other hand, we observe - entry - having a lesser document frequency, meaning that it appears less often, while still maintaining a decent log-frequency implying that it is specif to some documents only. 

```{r}
Smartphone_reviews.freq %>% 
  ggplot(aes(x = log10(docfreq),
             y = log10(frequency))) + 
  geom_text(aes(label=feature),
            position=position_jitter(),
            size = 3) + 
  xlab("Document log-frequency") + 
  ylab("log-frequency")
```


#### Words Cloud

Another representation of the Document frequency matrix is through a word cloud, where the the most relevant words has the biggest size. From this cloud, we can identify - phone, battery, screen, iPhone, scratch, condition - as the most used words.

```{r}
textplot_wordcloud(Smartphone_reviews.dfm)
```


#### Lexical diversity

We now want to take a glance a the diversity of words used. This is an interesting approach as it allows us to see if reviews are rich in vocabulary or if instead, they are repetitive. Due to the number of documents, we will only show a sample. A limitation of this representation is that the lexical diversity is dependent on the size of the sentence, in this case the reviews. Nonetheless it give us insight on how diverse the reviewers is in term of words used. The highest the TTR the more diverse the lexical is. 

```{r}
Smartphone_reviews.dfm %>% textstat_lexdiv() %>% 
  head(10) %>%
  ggplot(aes(reorder(document, -TTR),
             TTR)) + 
  geom_bar(stat="identity") +
  xlab("Text") +
  theme(axis.text.x = element_text(angle = 45, vjust = 0.5, hjust=0.5),
        strip.text = element_text(size=7)) 

# Lexical diversity per document(Model grouped)
Smartphone_reviews.dfm.group %>% textstat_lexdiv() %>% 
  head(10) %>%
  ggplot(aes(reorder(document, -TTR),
             TTR)) + 
  geom_bar(stat="identity") +
  xlab("Text") +
  theme(axis.text.x = element_text(angle = 45, vjust = 0.5, hjust=0.5),
        strip.text = element_text(size=7)) 

```

#### Keyness

To continue with our analysis we decided to applied a Chi-square test of independence between Apple's and Samsung's reviews. The purpose of this test is to compare term from a set of document to another, in this case we want to put in perspective terms used from one consumers base to an others. We create a plot with the keyness results, we also added for reference two tables containing the 10 first values from the Chi-square test for both Samsung and Apple as target. 

From the graph, we can see that both reviews will have as most common words used their respective brand. The main difference lays in the following exclusive words used. It seems that for  Samsung most of the term are related to other models (Apple as reference). Meanwhile, if Apple becomes the target (Samsung as reference), it appears that the most common terms used are - condition, and scratches.

Our intuition behind this pattern, could be due for Samsung to the amount of product they proposed, thus reviewers have a larger set of comparison when reviewing:

* Galaxy-S
* Galaxy-A
* Galaxy-Z
* Galaxy-Foldables
* Galaxy-Notes

as for Apple:

* Iphone 
* Iphone mini
* Iphone Pro
* Iphone SE

```{r}
## Key terms of Apple vs Samsung
## Take a sub-corpus and clean it
Smartphone_reviews.tk2 <- tokens(corpus_subset(
  Smartphone_reviews.cp,
  Brand %in% c("Apple", "Samsung")),
  remove_punct = TRUE,
  remove_symbols = TRUE,
  remove_numbers = TRUE) %>% 
  tokens_tolower()  %>%  
  tokens_remove(pattern = stop_words$word)

## table
Smartphone_reviews.tk2 %>% 
  dfm() %>% 
  dfm_group(Brand) %>% 
  textstat_keyness(target = "Samsung") %>%
  head(10)

Smartphone_reviews.tk2 %>% 
  dfm() %>% 
  dfm_group(Brand) %>% 
  textstat_keyness(target = "Apple") %>%
  head(10)

## Compute and show key terms
Smartphone_reviews.tk2 %>% 
  dfm() %>% 
  dfm_group(Brand) %>% 
  textstat_keyness(target = "Samsung") %>%
  textplot_keyness()

```

#### Link between words

To get an idea of the relationship between words - what are the most common combination - we decided to present a visual representation of those links. From the following graph we interpret that the majority of the most common words are linked directly with the term "phone" - this of course makes sense as we are working on phones reviews. It can be noted however that on the outer reach of this plot terms such as - perfect, excellent, recommend, happy - are reviews that mostly appears only with one token. This could explain the reason why on this graph these terms do not appear with links to others tokens. 

```{r}
## Represent some links between terms
smartphone_reviews.co <- fcm(Smartphone_reviews.tk, 
                 context = "document",
                 tri = FALSE)
index <- Smartphone_reviews.freq %>% 
  filter(frequency > 800) %>% 
  data.frame() %>% 
  select(feature)
smartphone_reviews.co <- smartphone_reviews.co[index$feature, index$feature]
smartphone_reviews.co[smartphone_reviews.co <= 2000] <- 0
smartphone_reviews.co[smartphone_reviews.co > 2000] <- 1
network <- graph_from_adjacency_matrix(
  smartphone_reviews.co,
  mode = "undirected",
  diag = FALSE)
plot(network,
     layout = layout_with_kk)

```


### Topic Modeling

#### LSA on TF (DFM)

```{r, warning=FALSE}
Smartphone_reviews.lsa <- textmodel_lsa(
  x = Smartphone_reviews.dfm.group,
  nd = 10) 

head(Smartphone_reviews.lsa$docs)
head(Smartphone_reviews.lsa$features)

n.terms <- 5

## For Dimension 2
w.order <- sort(Smartphone_reviews.lsa$features[, 2], decreasing = TRUE)
w.top2 <- c(w.order[1:n.terms], rev(rev(w.order)[1:n.terms]))
## For Dimension 3
w.order <- sort(Smartphone_reviews.lsa$features[,3], decreasing = TRUE)
w.top3 <- c(w.order[1:n.terms], rev(rev(w.order)[1:n.terms]))

w.top2
w.top3

w.subset <- Smartphone_reviews.lsa$features[
    c(unique(c(names(w.top2), names(w.top3)))), 2:3]

biplot(
  y = Smartphone_reviews.lsa$docs[, 2:3],
  x = w.subset,
  col = c("black","red"),
  cex = c(0.8, 0.8),
  xlab = "Dim 2",
  ylab = "Dim 3")
```

#### LSA on TF (TF-IDF)

```{r, warning=FALSE}
Smartphone_reviews.lsa2 <- textmodel_lsa(Smartphone_reviews.tfidf.group, 
                                         nd = 10) 
head(Smartphone_reviews.lsa2$docs)
head(Smartphone_reviews.lsa2$features)
Smartphone_reviews.lsa2$sk

n.terms <- 5
w.order <- sort(Smartphone_reviews.lsa2$features[,2], decreasing = TRUE)
w.top2 <- c(w.order[1:n.terms], rev(rev(w.order)[1:n.terms]))

w.order <- sort(Smartphone_reviews.lsa2$features[,3], decreasing = TRUE)
w.top3 <- c(w.order[1:n.terms], rev(rev(w.order)[1:n.terms]))

w.subset <- Smartphone_reviews.lsa2$features[c(unique(c(names(w.top2), names(w.top3)))),2:3]
biplot(
  y = Smartphone_reviews.lsa2$docs[,2:3],
  x = w.subset,
  col = c("black", "red"),
  cex = c(0.8, 0.8),
  xlab = "Dim 2",
  ylab="Dim 3")
```


#### LDA Using quanteda

```{r}
# Term-Topic Analysis
library(seededlda)

set.seed(123)
Smartphone_reviews.lda <- textmodel_lda(x = Smartphone_reviews.dfm.group, k = 5)
seededlda::terms(Smartphone_reviews.lda, 5)
seededlda::topics(Smartphone_reviews.lda)
seededlda::topics(Smartphone_reviews.lda) %>% table()

# Term-Topic Analysis
phi.long <- melt(
  Smartphone_reviews.lda$phi,
  varnames = c("Topic", "Term"),
  value.name = "Phi") 

p1 <- phi.long %>% 
  group_by(Topic) %>% 
  top_n(10, Phi) %>% 
  ggplot(aes(reorder_within(Term, Phi, Topic), Phi, fill=Topic)) + 
  geom_col(show.legend = FALSE) +
  coord_flip() +
  facet_wrap(~ Topic, scales = "free_y") +
  scale_x_reordered() + 
  xlab("Term") + 
  theme(
    axis.text.y = element_text(size = 8),
    strip.text = element_text(size = 8))

# Topic-Document Analysis

set.seed(123)
theta.long <- melt(
  Smartphone_reviews.lda$theta,
  varnames = c("Doc", "Topic"),
  value.name = "Theta")

p2 <- theta.long %>% 
  group_by(Topic) %>% 
  top_n(10, Theta) %>% 
  ggplot(aes(reorder_within(Doc, Theta, Topic), Theta, fill=Topic)) + 
  geom_col(show.legend = FALSE) +
  coord_flip()+
  facet_wrap(~ Topic, scales = "free_y") +
  scale_x_reordered() + 
  xlab("Document") + 
  theme(
    axis.text.y = element_text(size = 8),
    strip.text = element_text(size = 8))

p1/p2



ggplot(theta.long,
       aes(x = Theta,
           y = Doc,
           fill = Topic)) + 
  geom_bar(stat="identity") +
  ylab("Model") 
```


##### LDA Diagnostics across Topics - Prevalent, Coherence and Exclusivity -

```{r}
# Most Prevalent Topic
rev(sort(colSums(Smartphone_reviews.lda$theta)/sum(Smartphone_reviews.lda$theta)))

# Topic Coherence
speech.codo <- fcm(
  Smartphone_reviews.dfm.group, 
  context = "document",
  count = "boolean",
  tri = FALSE) # co-document frequencies
term.mat <- seededlda::terms(Smartphone_reviews.lda, 5)
Coh <- rep(0, 5)
names(Coh) <- paste0("Topic", 1:5)
for (k in 1:5) {
  D.mat <- t(speech.codo[term.mat[,k], term.mat[,k]])
  D.vec <- Smartphone_reviews.dfm.group %>% 
    textstat_frequency %>% 
    filter(feature %in% term.mat[, k]) %>% 
    data.frame %>%
    select(feature, docfreq)
  for (m in 2:5){
    for (l in 1:(m - 1)) {
      vm <- term.mat[m, k]
      vl <- term.mat[l, k]
      Coh[k] <- Coh[k] + log((D.mat[vm, vl] + 1) / filter(D.vec, feature == vl)$docfreq)
    }
  }
}
rev(sort(Coh))

# See coherent Topic
as.matrix(speech.codo[term.mat[, 2], term.mat[, 2]])

# See least coherent Topic
as.matrix(speech.codo[term.mat[, 1], term.mat[, 1]])


# Exclusivity of the topic
excl <- rep(0, 5)
names(excl) <- paste0("Topic", 1:5)
for (k in 1:5) {
  for (i in 1:length(term.mat[,k])) {
    term.phi <- filter(phi.long, Term == term.mat[i,k])
    excl[k] <- excl[k] + filter(term.phi, Topic == "topic1")$Phi / sum(term.phi$Phi)
  }
  excl[k] <- excl[k] / length(term.mat[, k])
}
rev(sort(excl))
```


