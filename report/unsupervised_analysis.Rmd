---
output: html_document
editor_options: 
  chunk_output_type: inline
---

```{r, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, message=FALSE, cache=FALSE, warning=FALSE)
source(here::here("script/setup.R"))
```

## **Unsupervised Learning**

### **Similarities**

For the unsupervised learning analysis, we will be analyzing the similarities between each of the models and the similarities between words that are present in the reviews. Out of the three distance measurements, we will be using Euclidean Distance as our distance/similarity measurement for this project as it compares the shortest distance among objects.

```{r}
smartphone.euc <- textstat_dist(
  Smartphone_reviews.tfidf.group,
  method = "euclidean",
  margin = "documents")
```

To read the euclidean distance from the matrix is quite complex since there are several models. Therefore, to highlight similarities in an easier way, we create a heatmap representation of the similarities between the reviews.

Looking at the heatmap of the grouped reviews below, we can conclude the following points:

-   iPhone 11 is not similar to any other phone. The closest one to it is the iPhone 11 Pro where the euclidean distance is around 0.5.
-   Samsung Galaxy S21 FE is not similar to any other phone
-   Samsung Galaxy S22 and S22 Plus are the ones that are similar to almost all the remaining phone models, excpet of course to iPhone 11 and 11 Pro.

Further, we have studied the graph that plots the euclidean distances based on the thickness of the line connecting them. The thicker the line, the more dissimilar or further the models are. Therefore, as we have seen in the heat map earlier and based on the thickness of the lines we see, this graphs also confirms our previous findings that Samsung Galaxy S21 FE and iPhone 11 are the most dissimilar models from all the other models.

```{r}
## Euclidean

# Since we are taking the Euclidean distance, we will first need to transform the distances to similarities bounded between 0-1. 
smartphone.euc.matrix <- melt(as.matrix(smartphone.euc))
max_dist <- max(smartphone.euc.matrix$value) # to get the maximum distance
smartphone.euc.matrix$value.std <- (max_dist - smartphone.euc.matrix$value)/max_dist 

#Heatmap
ggplotly(ggplot(
  data = smartphone.euc.matrix,
  mapping = aes(x = Var1, 
                y = Var2,
                fill = value.std)) +
  scale_fill_gradient2(
    low = "blue",
    high = "red",
    mid = "white", 
    midpoint = 0.5,
    limit = c(0, 1),
    name = "Similarity") +
  geom_tile() + xlab("") + ylab("") +
  theme(axis.text.x = element_text(angle = 90, vjust = 0.2, hjust=0.95),
        strip.text = element_text(size=7)))


library(qgraph)

qgraph(as.matrix(smartphone.euc), shape= "circle", posCol= "darkgreen", negCol="darkred", layout="groups", vsize=10)

```

### **Clustering of Documents**

Moving on to the Clustering of Documents, we have created some dendrograms to show which models are similar to each other and at which stage are they clustered together. We can see here that one of the first clusters that was made was between iPhone 13 Pro and Samsung Galaxy 21 Ultra. Further, as you can see, we can choose to have four (4) clusters. Comparing the results of the dendrogram to our previous results, we can also confirm the dissimilarity level between the models and iPhone 11 and Samsung Galaxy S21 FE as they were added to the cluster in the last two iterations.

For the clustering below, we have chosen the complete linkage method as it provides almost the same results as average linkage method and shows clearly distic=nct clusters. 

```{r}
smartphone.hc <- hclust(as.dist(smartphone.euc), method = "complete")
plot(smartphone.hc)
```

```{r, include=FALSE}

smartphone.clust <- cutree(smartphone.hc, k = 4)
smartphone.clust


smartphone.km <- kmeans(Smartphone_reviews.tfidf.group, centers = 4)
smartphone.km$cluster
```

Analyzing the results of the kmeasns on 4 clusters, we can see that we have the ration of Between Sum of Squares to the Total Sum of Squares equal to around 90% making the ratio of Within Sum of Squares equal to 10%. These results look promising as we would like to increase the Between Sum of Squares and decrease the Within Sum of Squares
<br>

##### Extract the ten words that are the most used.

We have then created the 4 clusters and have chosen to extract the 10 words that are more often used in each cluster to get an insight on what each cluster talks about. 

```{r}
data.frame(
  Clust.1 = names(sort(apply(Smartphone_reviews.tfidf.group[smartphone.clust==1, ], 2, sum), decreasing = TRUE)[1:10]),
  Clust.2 = names(sort(apply(Smartphone_reviews.tfidf.group[smartphone.clust==2, ], 2, sum), decreasing = TRUE)[1:10]),
  Clust.3 = names(sort(apply(Smartphone_reviews.tfidf.group[smartphone.clust==3, ], 2, sum), decreasing = TRUE)[1:10]), 
  Clust.4 = names(sort(apply(Smartphone_reviews.tfidf.group[smartphone.clust==4, ], 2, sum), decreasing = TRUE)[1:10])
)


data.frame(
  Clust.1 = names(sort(apply(Smartphone_reviews.tfidf.group[smartphone.km$cluster==1,], 2, sum), decreasing = TRUE)[1:10]),
  Clust.2 = names(sort(apply(Smartphone_reviews.tfidf.group[smartphone.km$cluster==2,], 2, sum), decreasing = TRUE)[1:10]),
  Clust.3 = names(sort(apply(Smartphone_reviews.tfidf.group[smartphone.km$cluster==3,], 2, sum), decreasing = TRUE)[1:10]),
  Clust.4 = names(sort(apply(Smartphone_reviews.tfidf.group[smartphone.km$cluster==4,], 2, sum), decreasing = TRUE)[1:10])
)
```


#### **Similarities between Words**

Then we do the similarities by words. We also create a heat map to show the similarities between words. Surprisingly, the heat map below does not show similarities between any of the words. The closest similarity between two words was between "life" and "saver" with around 0.55 cosine angle which makes sense since those two (2) words sometimes come together.


Then we do the similarities by words. We also create a heat map to show the similarities between words. The heat map below shows similarities between all of the words used in the reviews. However, the word "samsung" has the most dissimilarities with the others compared to the other words. Looking into details, to the similarities of the word "samsung", we can see that the most similar words to it are "camera" and "fast". Therefore, we can say that, between all the features, what most people commented about in the samsung phone is it's speed and it's camera. 

<br>

On the other hand, if we look at the similarity matrix for the iPhone, we can see that is is similar to almost all the features that were mentioned which means that the consumers have mentioned those features evenly in their reviews. However, it is least similar to the words, "camera", "fast", and "card". Which means that these words were not mentioned a lot in the iPhone reviews with respect to other words.  

```{r}
smartphone.feat <- textstat_frequency(Smartphone_reviews.dfm.group) %>% 
  filter(rank <= 50) # words with frequency rank less than 40 (it should correspond to the 50 most frequent words 
smartphone.feat$feature

smartphone.word.cos <- textstat_simil(
  Smartphone_reviews.dfm.group[, smartphone.feat$feature],
  method = "cosine",
  margin = "feature")
smartphone.word.cos.matrix <- melt(as.matrix(smartphone.word.cos)) # Convert the object to matrix then to data frame 

ggplotly(ggplot(data = smartphone.word.cos.matrix, aes(x=Var1, y=Var2, fill=value)) +
  scale_fill_gradient2(
    low = "blue",
    high = "red",
    mid = "white",
    midpoint = 0.5,
    limit = c(0, 1),
    name = "Cosine") +
  geom_tile() + 
  theme(
    axis.text.x = element_text(angle = 45, hjust = 1, size = 8),
    axis.text.y = element_text(size = 8)) +
  xlab("") + 
  ylab(""))
```

#### Clustering Words

The below dendrogram shows how each word is clustered and with the other words. we can c;early see four (4) clusters. As seen previously in the heatmap, the word samsung will be clustered in a cluster by itself as it is the most dissimilar word from the others.

```{r}
smartphone.word.hc <- hclust(as.dist(1 - smartphone.word.cos), method = "complete")
plot(smartphone.word.hc)
```

#### Cooccurence

Cooccurence describes how words occur together which in turn captures the different relationships between words. From there we can see thatthe most coocuuring words together are the words "phone" and "battery". We can also clearly see that from the dendrogram. 

```{r}

#### TO CHECK IF WE SHOULD USE THE GROUPED ONE ####

Smartphone_reviews.tk.group <- tokens(
  Smartphone_reviews.cp.group,
  remove_punct = TRUE,
  remove_symbols = TRUE,
  remove_numbers = TRUE,
  remove_separators = TRUE) %>%
  tokens_tolower() %>%
  tokens_remove(stop_words$word)


smartphone.fcm <- fcm(Smartphone_reviews.tk.group, 
                 window = 3, 
                 tri = FALSE)
smartphone.fcm <- (smartphone.fcm + t(smartphone.fcm))/2 
```

```{r}
# heat map of the most frequent features

smartphone.fcm.mat <- melt(
  as.matrix(
    smartphone.fcm[smartphone.feat$feature, smartphone.feat$feature]),
  varnames = c("Var1", "Var2")) 
ggplotly(ggplot(data = smartphone.fcm.mat, 
       mapping = aes(x = Var1, y = Var2, fill = value)) +
  scale_fill_gradient2(
    low = "blue",
    high = "red",
    mid = "white",
    midpoint = 140,
    limit = c(0, 280),
    name = "Co-occurrence") +
  geom_tile() + 
  theme(axis.text.x = element_text(angle = 45, hjust = 1, size = 5),
        axis.text.y = element_text(size = 5)) +
  xlab("") +
  ylab(""))

```

```{r}
# Produce clustering analysis. The co-occurrences are turned into dissimilarities before

smartphone.inv_occ <- 
  280 - as.matrix(
    smartphone.fcm[smartphone.feat$feature, smartphone.feat$feature]) ## 280 is the max co-occurrence here

smartphone.occ.hc <- hclust(as.dist(smartphone.inv_occ), method = "complete")
plot(smartphone.occ.hc)
```

### **Topic Modeling**

We decided to use Topic modeling to discover the abstract "topics" that occur in a collection of documents, in this case the grouped corpus of smartphones Models (*Wikipedia,2022*). Topic Modeling will help us to identify the context of the documents by detecting similar words patterns inside them, and by clustering those group of words together.

#### **LSA on TF (DFM)**

Latent Semantic Analysis, or LSA, is one of the techniques that we will use for topic modeling. LSA is a reduction technique that decomposes the DTM into 3 matrices ($M = ð‘ˆÎ£ð‘‰^{ð‘¡}$), where $Î£$ represents the strength of the topic, $ð‘ˆ$ the links between the document and each topic, and $ð‘‰^{t}$ the links between the terms and each topic.



```{r, warning=FALSE}
Smartphone_reviews.lsa <- textmodel_lsa(
  x = Smartphone_reviews.dfm.group,
  nd = 5) 

# Ddefine the number of terms to filter
n.terms <- 5

# For Dimension 1
w.order <- sort(Smartphone_reviews.lsa$features[, 1], decreasing = TRUE)
w.top1 <- c(w.order[1:n.terms], rev(rev(w.order)[1:n.terms]))

# For Dimension 2
w.order <- sort(Smartphone_reviews.lsa$features[, 2], decreasing = TRUE)
w.top2 <- c(w.order[1:n.terms], rev(rev(w.order)[1:n.terms]))

# For Dimension 3
w.order <- sort(Smartphone_reviews.lsa$features[,3], decreasing = TRUE)
w.top3 <- c(w.order[1:n.terms], rev(rev(w.order)[1:n.terms]))

# For Dimension 4
w.order <- sort(Smartphone_reviews.lsa$features[, 4], decreasing = TRUE)
w.top4 <- c(w.order[1:n.terms], rev(rev(w.order)[1:n.terms]))

# For Dimension 5
w.order <- sort(Smartphone_reviews.lsa$features[, 5], decreasing = TRUE)
w.top5 <- c(w.order[1:n.terms], rev(rev(w.order)[1:n.terms]))

w.top1
w.top2
w.top3
w.top4
w.top5
```


```{r}
w.subset <- Smartphone_reviews.lsa$features[
    c(unique(c(names(w.top2), names(w.top3)))), 2:3]

biplot(
  y = Smartphone_reviews.lsa$docs[, 2:3],
  x = w.subset,
  col = c("black","red"),
  cex = c(0.8, 0.8),
  xlab = "Dim 2",
  ylab = "Dim 3")

w.subset <- Smartphone_reviews.lsa$features[
    c(unique(c(names(w.top5), names(w.top5)))), 4:5]

biplot(
  y = Smartphone_reviews.lsa$docs[, 4:5],
  x = w.subset,
  col = c("black","red"),
  cex = c(0.8, 0.8),
  xlab = "Dim 4",
  ylab = "Dim 5")
```

#### **LSA on TF (TF-IDF)**

```{r, warning=FALSE}
Smartphone_reviews.lsa2 <- textmodel_lsa(Smartphone_reviews.tfidf.group, 
                                         nd = 10) 
head(Smartphone_reviews.lsa2$docs)
head(Smartphone_reviews.lsa2$features)
Smartphone_reviews.lsa2$sk

n.terms <- 5
w.order <- sort(Smartphone_reviews.lsa2$features[,2], decreasing = TRUE)
w.top2 <- c(w.order[1:n.terms], rev(rev(w.order)[1:n.terms]))

w.order <- sort(Smartphone_reviews.lsa2$features[,3], decreasing = TRUE)
w.top3 <- c(w.order[1:n.terms], rev(rev(w.order)[1:n.terms]))

w.subset <- Smartphone_reviews.lsa2$features[c(unique(c(names(w.top2), names(w.top3)))),2:3]
biplot(
  y = Smartphone_reviews.lsa2$docs[,2:3],
  x = w.subset,
  col = c("black", "red"),
  cex = c(0.8, 0.8),
  xlab = "Dim 2",
  ylab="Dim 3")
```

#### **LDA** 

```{r}
# Term-Topic Analysis
set.seed(123)
Smartphone_reviews.lda <- textmodel_lda(x = Smartphone_reviews.dfm.group, k = 5)
seededlda::terms(Smartphone_reviews.lda, 5)
seededlda::topics(Smartphone_reviews.lda)
seededlda::topics(Smartphone_reviews.lda) %>% table()

# Term-Topic Analysis
phi.long <- melt(
  Smartphone_reviews.lda$phi,
  varnames = c("Topic", "Term"),
  value.name = "Phi") 

p1 <- phi.long %>% 
  group_by(Topic) %>% 
  top_n(10, Phi) %>% 
  ggplot(aes(reorder_within(Term, Phi, Topic), Phi, fill=Topic)) + 
  geom_col(show.legend = FALSE) +
  coord_flip() +
  facet_wrap(~ Topic, scales = "free_y") +
  scale_x_reordered() + 
  xlab("Term") + 
  theme(
    axis.text.y = element_text(size = 8),
    strip.text = element_text(size = 8))

# Topic-Document Analysis

set.seed(123)
theta.long <- melt(
  Smartphone_reviews.lda$theta,
  varnames = c("Doc", "Topic"),
  value.name = "Theta")

p2 <- theta.long %>% 
  group_by(Topic) %>% 
  top_n(10, Theta) %>% 
  ggplot(aes(reorder_within(Doc, Theta, Topic), Theta, fill=Topic)) + 
  geom_col(show.legend = FALSE) +
  coord_flip()+
  facet_wrap(~ Topic, scales = "free_y") +
  scale_x_reordered() + 
  xlab("Document") + 
  theme(
    axis.text.y = element_text(size = 8),
    strip.text = element_text(size = 8))

p1/p2



ggplot(theta.long,
       aes(x = Theta,
           y = Doc,
           fill = Topic)) + 
  geom_bar(stat="identity") +
  ylab("Model") 
```

##### **LDA Diagnostics across Topics - Prevalent, Coherence and Exclusivity -**

```{r}
# Most Prevalent Topic
rev(sort(colSums(Smartphone_reviews.lda$theta)/sum(Smartphone_reviews.lda$theta)))

# Topic Coherence
speech.codo <- fcm(
  Smartphone_reviews.dfm.group, 
  context = "document",
  count = "boolean",
  tri = FALSE) # co-document frequencies
term.mat <- seededlda::terms(Smartphone_reviews.lda, 5)
Coh <- rep(0, 5)
names(Coh) <- paste0("Topic", 1:5)
for (k in 1:5) {
  D.mat <- t(speech.codo[term.mat[,k], term.mat[,k]])
  D.vec <- Smartphone_reviews.dfm.group %>% 
    textstat_frequency %>% 
    filter(feature %in% term.mat[, k]) %>% 
    data.frame %>%
    select(feature, docfreq)
  for (m in 2:5){
    for (l in 1:(m - 1)) {
      vm <- term.mat[m, k]
      vl <- term.mat[l, k]
      Coh[k] <- Coh[k] + log((D.mat[vm, vl] + 1) / filter(D.vec, feature == vl)$docfreq)
    }
  }
}
rev(sort(Coh))

# See coherent Topic
as.matrix(speech.codo[term.mat[, 2], term.mat[, 2]])

# See least coherent Topic
as.matrix(speech.codo[term.mat[, 1], term.mat[, 1]])


# Exclusivity of the topic
excl <- rep(0, 5)
names(excl) <- paste0("Topic", 1:5)
for (k in 1:5) {
  for (i in 1:length(term.mat[,k])) {
    term.phi <- filter(phi.long, Term == term.mat[i,k])
    excl[k] <- excl[k] + filter(term.phi, Topic == "topic1")$Phi / sum(term.phi$Phi)
  }
  excl[k] <- excl[k] / length(term.mat[, k])
}
rev(sort(excl))
```
