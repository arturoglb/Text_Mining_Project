---
output: html_document
editor_options: 
  chunk_output_type: console
---

```{r, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, message=FALSE, cache=FALSE, warning=FALSE)
source(here::here("script/setup.R"))
```

## **Unsupervised Learning**

### **Similarities**

For this section, we will be analyzing the similarities between each of the documents and the similarities between words that are present in the reviews. Out of the three distance measurements, we will be using Euclidean Distance as our distance/similarity measurement for this project as it compares the shortest distance among objects.

```{r}
# compute the Jaccard index matrix, the cosine matrix, and the Euclidean distances matrix.
# smartphone.jac <- textstat_simil(
#   Smartphone_reviews.tfidf,
#   method = "jaccard",
#   margin = "documents")
# 
# smartphone.cos <- textstat_simil(
#   Smartphone_reviews.tfidf.group,
#   method = "cosine",
#   margin = "documents")

smartphone.euc <- textstat_dist(
  Smartphone_reviews.tfidf.group,
  method = "euclidean",
  margin = "documents")

```

To highlight similarities in an easier way, we create a heatmap representation of the similarities between the reviews.

Looking at the heatmap below, we can conclude the following points:

-   iPhone 11 is not similar to any other phone. The closest one to it is the iPhone 11 Pro where the euclidean distance is around 0.5.
-   Samsung Galaxy S21 FE is not similar to any other phone
-   Samsung Galaxy S22 and S22 Plus are the ones that are similar to almost all the remaining phone models.

Further, we have seen the graph that plots the euclidean distances based on the thickness of the line connecting them. The thicker the line, the more dissimilar the models are. Therefore, as we have seen in the heat map earlier and based on the thickness of the lines we see, this graphs also confirms our previous findings that Samsung Galaxy S21 FE and iPhone 11 are the more dissimilar models from all the other models.

```{r}
# make a heatmap representation of the similarities between the documents.

# ## Jaccard 
# smartphone.jac.matrix <- melt(as.matrix(smartphone.jac)) # Convert the object to matrix then to data frame 
# ggplot(data = smartphone.jac.matrix, 
#        mapping = aes(x = Var1, y = Var2, fill = value)) +
#   scale_fill_gradient2(
#     low = "blue",
#     high = "red",
#     mid = "white", 
#     midpoint = 0.5,
#     limit = c(0, 1),
#     name = "Jaccard") +
#   geom_tile() + xlab("") + ylab("")
# 
# 
# ## Cosine
# smartphone.cos.matrix <- melt(as.matrix(smartphone.cos))
# ggplot(
#   data = smartphone.cos.matrix,
#   mapping = aes(x = Var1, y = Var2, fill = value)) +
#   scale_fill_gradient2(
#     low = "blue",
#     high = "red",
#     mid = "white",
#     midpoint = 0.5,
#     limit = c(0, 1),
#     name = "Cosine") +
#   geom_tile() + xlab("") + ylab("")


## Euclidean

# Since we are taking the Euclidean distance, we will first need to transform the distances to similarities bounded between 0-1. 
smartphone.euc.matrix <- melt(as.matrix(smartphone.euc))
max_dist <- max(smartphone.euc.matrix$value) # to get the maximum distance
smartphone.euc.matrix$value.std <- (max_dist - smartphone.euc.matrix$value)/max_dist 

#Heatmap
ggplot(
  data = smartphone.euc.matrix,
  mapping = aes(x = Var1, 
                y = Var2,
                fill = value.std)) +
  scale_fill_gradient2(
    low = "blue",
    high = "red",
    mid = "white", 
    midpoint = 0.5,
    limit = c(0, 1),
    name = "Euclidean") +
  geom_tile() + xlab("") + ylab("") +
  theme(axis.text.x = element_text(angle = 90, vjust = 0.2, hjust=0.95),
        strip.text = element_text(size=7))


library(qgraph)

qgraph(as.matrix(smartphone.euc), shape= "circle", posCol= "darkgreen", negCol="darkred", layout="groups", vsize=10)

# tried this code to get the inverse of the matrix to make the thinkness meaning more similar but didn't work
# qgraph((1/as.matrix(smartphone.euc)), shape= "circle", posCol= "darkgreen", negCol="darkred", layout="groups", vsize=10)
```

### **Clustering of Documents**

Moving on to the Clustering of Documents, we have created some dendrograms to show which models are similar to each other and at which state. Further, as you can see, we can choose to have four (4) clusters. Comparing the results of the dendrogram to our previous results, we can also confirm the dissimilarity level between the models and iPhone 11 and Samsung Galaxy S21 FE as they were added to the cluster in the last two iterations.

```{r}
smartphone.hc <- hclust(as.dist(smartphone.euc))
## smartphone.hc <- hclust(as.dist(1 - smartphone.jac)) # use this line for Jaccard
## smartphone.hc <- hclust(as.dist(1 - smartphone.cos)) # use this line for Cosine
plot(smartphone.hc)
```

```{r}

smartphone.clust <- cutree(smartphone.hc, k = 4)
smartphone.clust


smartphone.km <- kmeans(Smartphone_reviews.tfidf.group, centers = 4)
smartphone.km$cluster
```

##### Extract the ten words that are the most used.

We have then created the 4 clusters and have chosen to extract the 10 words that are more often used

```{r}
data.frame(
  Clust.1 = names(sort(apply(Smartphone_reviews.tfidf.group[smartphone.clust==1, ], 2, sum), decreasing = TRUE)[1:10]),
  Clust.2 = names(sort(apply(Smartphone_reviews.tfidf.group[smartphone.clust==2, ], 2, sum), decreasing = TRUE)[1:10]),
  Clust.3 = names(sort(apply(Smartphone_reviews.tfidf.group[smartphone.clust==3, ], 2, sum), decreasing = TRUE)[1:10]), 
  Clust.4 = names(sort(apply(Smartphone_reviews.tfidf.group[smartphone.clust==4, ], 2, sum), decreasing = TRUE)[1:10])
)


data.frame(
  Clust.1 = names(sort(apply(Smartphone_reviews.tfidf.group[smartphone.km$cluster==1,], 2, sum), decreasing = TRUE)[1:10]),
  Clust.2 = names(sort(apply(Smartphone_reviews.tfidf.group[smartphone.km$cluster==2,], 2, sum), decreasing = TRUE)[1:10]),
  Clust.3 = names(sort(apply(Smartphone_reviews.tfidf.group[smartphone.km$cluster==3,], 2, sum), decreasing = TRUE)[1:10]),
  Clust.4 = names(sort(apply(Smartphone_reviews.tfidf.group[smartphone.km$cluster==4,], 2, sum), decreasing = TRUE)[1:10])
)
```

#### **Similarities between Words**

Then we do the similarities by words. We also create a heat map to show the similarities between words. Surprisingly, the heat map below does not show similarities between any of the words. The closest similarity between two words was between "life" and "saver" with around 0.55 cosine angle which makes sense since those two (2) words sometimes come together.



Then we do the similarities by words. We also create a heat map to show the similarities between words. The heat map below shows similarities between all of the words. However, the word "samsung" has the most dissimilarities with the others compared to the other words. 

```{r}
smartphone.feat <- textstat_frequency(Smartphone_reviews.dfm.group) %>% 
  filter(rank <= 40) # words with frequency rank less than 40 (it should correspond to the 40 most frequent words 
smartphone.feat$feature

smartphone.word.cos <- textstat_simil(
  Smartphone_reviews.dfm.group[, smartphone.feat$feature],
  method = "cosine",
  margin = "feature")
smartphone.word.cos.matrix <- melt(as.matrix(smartphone.word.cos)) # Convert the object to matrix then to data frame 

ggplot(data = smartphone.word.cos.matrix, aes(x=Var1, y=Var2, fill=value)) +
  scale_fill_gradient2(
    low = "blue",
    high = "red",
    mid = "white",
    midpoint = 0.5,
    limit = c(0, 1),
    name = "Cosine") +
  geom_tile() + 
  theme(
    axis.text.x = element_text(angle = 45, hjust = 1, size = 8),
    axis.text.y = element_text(size = 8)) +
  xlab("") + 
  ylab("")
```

#### Clustering Words


From the below dendrogram we can see that the first 2 words that were clustered were "life " and saver" and then "screen protector". It is worth mentioning that the dendrogram below is created from a dissimilarity matrix.  

From the below dendrogram we can see that the first 2 words that were clustered were "life" and saver" and then "screen protector". It is worth mentioning that the dendrogram below is created from a dissimilarity matrix.

```{r}
smartphone.word.hc <- hclust(as.dist(1 - smartphone.word.cos))
plot(smartphone.word.hc)
```

#### Cooccurence

Cooccurence describes how words occur together which in turn captures the different relationships between words.

```{r}

#### TO CHECK IF WE SHOULD USE THE GROUPED ONE ####


smartphone.fcm <- fcm(Smartphone_reviews.tk, 
                 window = 3, 
                 tri = FALSE)
smartphone.fcm <- (smartphone.fcm + t(smartphone.fcm))/2 
```

```{r}
# heat map of the most frequent features



smartphone.fcm.mat <- melt(
  as.matrix(
    smartphone.fcm[smartphone.feat$feature, smartphone.feat$feature]),
  varnames = c("Var1", "Var2")) 
ggplot(data = smartphone.fcm.mat, 
       mapping = aes(x = Var1, y = Var2, fill = value)) +
  scale_fill_gradient2(
    low = "blue",
    high = "red",
    mid = "white",
    midpoint = 140,
    limit = c(0, 280),
    name = "Co-occurrence") +
  geom_tile() + 
  theme(axis.text.x = element_text(angle = 45, hjust = 1, size = 5),
        axis.text.y = element_text(size = 5)) +
  xlab("") +
  ylab("")

```

```{r}
# Produce clustering analysis. The co-occurrences are turned into dissimilarities before

smartphone.inv_occ <- 
  280 - as.matrix(
    smartphone.fcm[smartphone.feat$feature, smartphone.feat$feature]) ## 280 is the max co-occurrence here

smartphone.occ.hc <- hclust(as.dist(smartphone.inv_occ))
plot(smartphone.occ.hc)
```

### **Topic Modeling**

We decided to use Topic modeling to discover the abstract "topics" that occur in a collection of documents, in this case the grouped corpus of smartphones Models (*Wikipedia,2022*). Topic Modeling will help us to identify the context of the documents by detecting similar words patterns inside them, and by clustering those group of words together.

#### **LSA on Term Frequencies (DFM)** {.tabset}

Latent Semantic Analysis, or LSA, is one of the techniques that we will use for topic modeling. LSA is a reduction technique that decomposes the DTM into 3 matrices ($M = 𝑈Σ𝑉^{𝑡}$), where $Σ$ represents the strength of the topic, $𝑈$ the links among the document and every topic, and $𝑉^{t}$ the links between the terms and each topic.

First, we started by plotting the first dimension to corroborate if it is associated with the document length, as it is known that this happens in LSA dim1. Looking at the result observed on the tab called Dimension 1, we can confirm that it is the case, as we detect that Dimension 1 is negatively correlated. Furthermore, we can see that the **iPhone 11** is in the bottom-right hand side of the chart being the one with the highest amount of tokens, while **Samsung Galaxy 21 Ultra** is located on the top-left hand side with the lowest amount.

On the second tab "Topics 2 and 3", we interpret which words are the top 5 associated to topics 2 and 3, and the top5 negatively associated to those topics. In the table for Topic 2 we detect that the words "scratch", "iphone", "condition", "battery" and "product" are the ones associated to this topic, while "5g", "s20", "camera", "phone" and "samsung" are negatively linked. We can say that Topic 2 can be identified on models from the brand Apple. On the other hand, in the table for Topic 3 we discover that the main words related to this topic were - pro, samsung, arrive, battery and camera - and the negative associated were brand, buy, iphone, unlock, and phone. This Topic may be seen on model reviews from the brand Samsung and some of the Pro models from Apple.

For a visual representation of the words stated previously on topic 2 and topic 3, we plot the dimensions that correspond to those topics (*Dim 2 & 3*) in a biplot chart. In the tab "Biplot of Dim 2 and 3" we can confirm the points mentioned before, as we note the same words associated with Dim 2 (*Topic 2*) and negatively associated, same case for Dim 3 (*Topic 3*). Samsung Galaxy S21 FE, iPhone 11 Pro Max and iPhone 11 Pro are associated with Topic 3, while iPhone 11 is unconnected to this topic. For Topic 2 we can determine that iPhone 11 Pro, iPhone 11 Pro Max, iPhone 11, and iPhone 12 are related to it. 

##### Dimension 1 {.active}

```{r, fig.width=8}
# Build LSA object
Smartphone_reviews.lsa <- textmodel_lsa(
  x = Smartphone_reviews.dfm.group,
  nd = 5) 

models.freq <- ntoken(Smartphone_reviews.tk.group)
lsa.dim1 <- data.frame(models.freq,
           dim1 = Smartphone_reviews.lsa$docs[, 1])
lsa.dim1$names <- rownames(lsa.dim1)
lsa.dim1 <- data.frame(lsa.dim1)

# Plot chart
plot_ly(data = lsa.dim1, x = ~models.freq, y = ~dim1, color = ~names,
        colors = "Set1", text = ~paste("Model: ", names)) %>% 
  layout(xaxis = list(title = 'Number of Tokens'), 
         yaxis = list(title = 'LSA Dim 1'),
         legend = list(title=list(text='<b> Smartphone Models </b>')))
```

##### Topics 2 and 3

```{r}
# Define the number of terms to filter
n.terms <- 5

# For Dimension 2
w.order <- sort(Smartphone_reviews.lsa$features[, 2], decreasing = TRUE)
w.top2 <- c(w.order[1:n.terms], rev(rev(w.order)[1:n.terms]))


# For Dimension 3
w.order <- sort(Smartphone_reviews.lsa$features[,3], decreasing = TRUE)
w.top3 <- c(w.order[1:n.terms], rev(rev(w.order)[1:n.terms]))

w.top2 %>% 
  kable(caption = "Topic 2") %>%
  kable_styling(bootstrap_options = c("striped","hover","condensed"),
                fixed_thead = T) 
w.top3 %>% 
  kable(caption = "Topic 3") %>%
  kable_styling(bootstrap_options = c("striped","hover","condensed"),
                fixed_thead = T) 
```

##### Biplot of Dim 2 and 3

```{r}
w.subset <- Smartphone_reviews.lsa$features[
    c(unique(c(names(w.top2), names(w.top3)))), 2:3]

biplot(
  y = Smartphone_reviews.lsa$docs[, 2:3],
  x = w.subset,
  col = c("black","red"),
  cex = c(0.8, 0.8),
  xlab = "Dim 2",
  ylab = "Dim 3")

```

## {-}

#### **LSA on Term Frequencies (TF-IDF)**

Now, we apply the same approach as above, but in this case we will use the TF-IDF matrix. As we have already explain, the TF-IDF quantifies the relevance of a word in a document. First, we build the LSA object with the `textmodel_lsa` function from the package `quanteda.textmodels` with our matrix created for the TF-IDF grouped model reviews(*only 5 dimensions*). Next, we break down the data for interpretability by considering only the 5 words with the highest values and the 5 with the lowest values. Finally, we plot the Biplot to identify the Topics and its words associated and unrelated.

What we discover is that Topic 2 (*Dim 2*) is associated to the words "scratch", "iphone", "generic", "health", "renew" and the models iPhone 11 Pro, iPhone 11, iPhone 11 Pro Max. Thereby, is unrelated to terms such as "s21", "s8", "samsung", "s20", "fe" and the model Samsung Galaxy S21 FE. For Topic 3 (*Dim 3*) we see a relation with terms like "aesthetic", "mica", "generic", "pro", "max" and 


```{r, warning=FALSE}
Smartphone_reviews.lsa2 <- textmodel_lsa(Smartphone_reviews.tfidf.group, 
                                         nd = 5) 
head(Smartphone_reviews.lsa2$docs)
head(Smartphone_reviews.lsa2$features)
Smartphone_reviews.lsa2$sk

n.terms <- 5
w.order <- sort(Smartphone_reviews.lsa2$features[,2], decreasing = TRUE)
w.top2 <- c(w.order[1:n.terms], rev(rev(w.order)[1:n.terms]))

w.order <- sort(Smartphone_reviews.lsa2$features[,3], decreasing = TRUE)
w.top3 <- c(w.order[1:n.terms], rev(rev(w.order)[1:n.terms]))

w.subset <- Smartphone_reviews.lsa2$features[c(unique(c(names(w.top2), names(w.top3)))),2:3]
biplot(
  y = Smartphone_reviews.lsa2$docs[,2:3],
  x = w.subset,
  col = c("black", "red"),
  cex = c(0.8, 0.8),
  xlab = "Dim 2",
  ylab="Dim 3")
```

#### **LDA** 

```{r}
# Term-Topic Analysis
set.seed(123)
Smartphone_reviews.lda <- textmodel_lda(x = Smartphone_reviews.dfm.group, k = 5)
seededlda::terms(Smartphone_reviews.lda, 5)
seededlda::topics(Smartphone_reviews.lda)
seededlda::topics(Smartphone_reviews.lda) %>% table()

# Term-Topic Analysis
phi.long <- melt(
  Smartphone_reviews.lda$phi,
  varnames = c("Topic", "Term"),
  value.name = "Phi") 

p1 <- phi.long %>% 
  group_by(Topic) %>% 
  top_n(10, Phi) %>% 
  ggplot(aes(reorder_within(Term, Phi, Topic), Phi, fill=Topic)) + 
  geom_col(show.legend = FALSE) +
  coord_flip() +
  facet_wrap(~ Topic, scales = "free_y") +
  scale_x_reordered() + 
  xlab("Term") + 
  theme(
    axis.text.y = element_text(size = 8),
    strip.text = element_text(size = 8))

# Topic-Document Analysis

set.seed(123)
theta.long <- melt(
  Smartphone_reviews.lda$theta,
  varnames = c("Doc", "Topic"),
  value.name = "Theta")

p2 <- theta.long %>% 
  group_by(Topic) %>% 
  top_n(10, Theta) %>% 
  ggplot(aes(reorder_within(Doc, Theta, Topic), Theta, fill=Topic)) + 
  geom_col(show.legend = FALSE) +
  coord_flip()+
  facet_wrap(~ Topic, scales = "free_y") +
  scale_x_reordered() + 
  xlab("Document") + 
  theme(
    axis.text.y = element_text(size = 8),
    strip.text = element_text(size = 8))

p1/p2



ggplot(theta.long,
       aes(x = Theta,
           y = Doc,
           fill = Topic)) + 
  geom_bar(stat="identity") +
  ylab("Model") 
```

##### **LDA Diagnostics across Topics - Prevalent, Coherence and Exclusivity -**

```{r}
# Most Prevalent Topic
rev(sort(colSums(Smartphone_reviews.lda$theta)/sum(Smartphone_reviews.lda$theta)))

# Topic Coherence
speech.codo <- fcm(
  Smartphone_reviews.dfm.group, 
  context = "document",
  count = "boolean",
  tri = FALSE) # co-document frequencies
term.mat <- seededlda::terms(Smartphone_reviews.lda, 5)
Coh <- rep(0, 5)
names(Coh) <- paste0("Topic", 1:5)
for (k in 1:5) {
  D.mat <- t(speech.codo[term.mat[,k], term.mat[,k]])
  D.vec <- Smartphone_reviews.dfm.group %>% 
    textstat_frequency %>% 
    filter(feature %in% term.mat[, k]) %>% 
    data.frame %>%
    select(feature, docfreq)
  for (m in 2:5){
    for (l in 1:(m - 1)) {
      vm <- term.mat[m, k]
      vl <- term.mat[l, k]
      Coh[k] <- Coh[k] + log((D.mat[vm, vl] + 1) / filter(D.vec, feature == vl)$docfreq)
    }
  }
}
rev(sort(Coh))

# See coherent Topic
as.matrix(speech.codo[term.mat[, 2], term.mat[, 2]])

# See least coherent Topic
as.matrix(speech.codo[term.mat[, 1], term.mat[, 1]])


# Exclusivity of the topic
excl <- rep(0, 5)
names(excl) <- paste0("Topic", 1:5)
for (k in 1:5) {
  for (i in 1:length(term.mat[,k])) {
    term.phi <- filter(phi.long, Term == term.mat[i,k])
    excl[k] <- excl[k] + filter(term.phi, Topic == "topic1")$Phi / sum(term.phi$Phi)
  }
  excl[k] <- excl[k] / length(term.mat[, k])
}
rev(sort(excl))
```
