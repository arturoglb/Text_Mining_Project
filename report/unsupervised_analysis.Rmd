---
output: html_document
editor_options: 
  chunk_output_type: inline
---
```{r, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, message=FALSE, cache=FALSE, warning=FALSE)
source(here::here("script/setup.R"))
```

## **Unsupervised Learning**


### **Similarities**

For this section, we will be analyzing the similarities between each of the documents and the similarities between words that are present in the reviews. Out of the three distance measurements, we will be using Euclidean Distance as our distance/similarity measurement for this project as it compares the shortest distance among objects. 

```{r}
# compute the Jaccard index matrix, the cosine matrix, and the Euclidean distances matrix.
# smartphone.jac <- textstat_simil(
#   Smartphone_reviews.tfidf,
#   method = "jaccard",
#   margin = "documents")
# 
# smartphone.cos <- textstat_simil(
#   Smartphone_reviews.tfidf,
#   method = "cosine",
#   margin = "documents")

smartphone.euc <- textstat_dist(
  Smartphone_reviews.tfidf.group,
  method = "euclidean",
  margin = "documents")

```

To highlight similarities in an easier way, we create a heatmap representation of the similarities between the reviews. 

Looking at the heatmap below, we can conclude the following points:

* iPhone 11 is not similar to any other phone. The closest one to it is the iPhone 11 Pro where the euclidean distance is around 0.5. 
* Samsung Galaxy S21 FE is not similar to any other phone
* Samsung Galaxy S22 and S22 Plus are the ones that are similar to almost all the remaining phone models. 

Further, we have seen the qgraph that plots the eucledian distances based on the thikness of the line connecting them. The thicker the line, the more dissimilar the models are. Therefore, as we have seen in the heatmap earlier and based on the thinkness of the lines we see, this graphs also confirms our previous findings that Samsung Galaxy S21 FE and iPhone 11 are the more dissimilar models from all the other models. 

```{r}
# make a heatmap representation of the similarities between the documents.

# ## Jaccard 
# smartphone.jac.matrix <- melt(as.matrix(smartphone.jac)) # Convert the object to matrix then to data frame 
# ggplot(data = smartphone.jac.matrix, 
#        mapping = aes(x = Var1, y = Var2, fill = value)) +
#   scale_fill_gradient2(
#     low = "blue",
#     high = "red",
#     mid = "white", 
#     midpoint = 0.5,
#     limit = c(0, 1),
#     name = "Jaccard") +
#   geom_tile() + xlab("") + ylab("")
# 
# 
# ## Cosine
# smartphone.cos.matrix <- melt(as.matrix(smartphone.cos))
# ggplot(
#   data = smartphone.cos.matrix,
#   mapping = aes(x = Var1, y = Var2, fill = value)) +
#   scale_fill_gradient2(
#     low = "blue",
#     high = "red",
#     mid = "white",
#     midpoint = 0.5,
#     limit = c(0, 1),
#     name = "Cosine") +
#   geom_tile() + xlab("") + ylab("")


## Euclidean

# Since we are taking the Euclidean distance, we will first need to transform the distances to similarities bounded between 0-1. 
smartphone.euc.matrix <- melt(as.matrix(smartphone.euc))
max_dist <- max(smartphone.euc.matrix$value) # to get the maximum distance
smartphone.euc.matrix$value.std <- (max_dist - smartphone.euc.matrix$value)/max_dist 

#Heatmap
ggplot(
  data = smartphone.euc.matrix,
  mapping = aes(x = Var1, 
                y = Var2,
                fill = value.std)) +
  scale_fill_gradient2(
    low = "blue",
    high = "red",
    mid = "white", 
    midpoint = 0.5,
    limit = c(0, 1),
    name = "Euclidean") +
  geom_tile() + xlab("") + ylab("") +
  theme(axis.text.x = element_text(angle = 90, vjust = 0.2, hjust=0.95),
        strip.text = element_text(size=7))


library(qgraph)

qgraph(as.matrix(smartphone.euc), shape= "circle", posCol= "darkgreen", negCol="darkred", layout="groups", vsize=10)

# tried this code to get the inverse of the matrix to make the thinkness meaning more similar but didn't work
# qgraph((1/as.matrix(smartphone.euc)), shape= "circle", posCol= "darkgreen", negCol="darkred", layout="groups", vsize=10)
```


### **Clustering of Documents**

Moving on to the Clustering of Documents, we have creted some dendrograms to show which models are similart to each other and at which state. Further, as you can see, we can choose to have four (4) clusters. Comparing the results of the dendrogram to our previous results, we can also confirm the dissimilarity level betweel the models and iPhone 11 and Samsung Galaxy S21 FE as they were added to the cluster in the last two iterations. 

```{r}
smartphone.hc <- hclust(as.dist(smartphone.euc))
## smartphone.hc <- hclust(as.dist(1 - smartphone.jac)) # use this line for Jaccard
## smartphone.hc <- hclust(as.dist(1 - smartphone.cos)) # use this line for Cosine
plot(smartphone.hc)
```

```{r}

smartphone.clust <- cutree(smartphone.hc, k = 4)
smartphone.clust


smartphone.km <- kmeans(Smartphone_reviews.tfidf.group, centers = 4)
smartphone.km$cluster
```

##### Extract the ten words that are the most used.

We have then created the 4 clusters and have chosen to extract the 10 words that are more often used 

```{r}
data.frame(
  Clust.1 = names(sort(apply(Smartphone_reviews.tfidf.group[smartphone.clust==1, ], 2, sum), decreasing = TRUE)[1:10]),
  Clust.2 = names(sort(apply(Smartphone_reviews.tfidf.group[smartphone.clust==2, ], 2, sum), decreasing = TRUE)[1:10]),
  Clust.3 = names(sort(apply(Smartphone_reviews.tfidf.group[smartphone.clust==3, ], 2, sum), decreasing = TRUE)[1:10]), 
  Clust.4 = names(sort(apply(Smartphone_reviews.tfidf.group[smartphone.clust==4, ], 2, sum), decreasing = TRUE)[1:10])
)


data.frame(
  Clust.1 = names(sort(apply(Smartphone_reviews.tfidf.group[smartphone.km$cluster==1,], 2, sum), decreasing = TRUE)[1:10]),
  Clust.2 = names(sort(apply(Smartphone_reviews.tfidf.group[smartphone.km$cluster==2,], 2, sum), decreasing = TRUE)[1:10]),
  Clust.3 = names(sort(apply(Smartphone_reviews.tfidf.group[smartphone.km$cluster==3,], 2, sum), decreasing = TRUE)[1:10]),
  Clust.4 = names(sort(apply(Smartphone_reviews.tfidf.group[smartphone.km$cluster==4,], 2, sum), decreasing = TRUE)[1:10])
)
```

#### **Similarities between Words**

Then we do the similarities by words. We also create a heat map to show the similarities between words. Surprisingly, the heat map below does not show similarities between any of the words. The closest similarity between two words was between "life" and "saver" with around 0.55 cosine angle which makes sense since those two (2) words sometimes come together. 

```{r}
smartphone.feat <- textstat_frequency(Smartphone_reviews.dfm.group) %>% #change this once R works
  filter(rank <= 40) # words with frequency rank less than 40 (it should correspond to the 40 most frequent words 
smartphone.feat$feature

smartphone.word.euc <- textstat_simil(
  Smartphone_reviews.dfm.group[, smartphone.feat$feature],
  method = "cosine",
  margin = "feature")
smartphone.word.cos.matrix <- melt(as.matrix(smartphone.word.cos)) # Convert the object to matrix then to data frame 

ggplot(data = smartphone.word.cos.matrix, aes(x=Var1, y=Var2, fill=value)) +
  scale_fill_gradient2(
    low = "blue",
    high = "red",
    mid = "white",
    midpoint = 0.5,
    limit = c(0, 1),
    name = "Cosine") +
  geom_tile() + 
  theme(
    axis.text.x = element_text(angle = 45, hjust = 1, size = 5),
    axis.text.y = element_text(size = 5)) +
  xlab("") + 
  ylab("")
```


#### Clustering Words

From the below dendrogram we can see that the first 2 words that were clistered wew "life " and saver" and then "screen protector". It is worth mentioning that the dendrogram below is created from a dissimilarity matrix.  

```{r}
smartphone.word.hc <- hclust(as.dist(1 - smartphone.word.cos))
plot(smartphone.word.hc)
```

#### Cooccurence

Cooccurence describes how words occur together which in turn captures the different relationships between words.

```{r}
smartphone.fcm <- fcm(Smartphone_reviews.tk, 
                 window = 3, 
                 tri = FALSE)
smartphone.fcm <- (smartphone.fcm + t(smartphone.fcm))/2 
```

```{r}
# heat map of the most frequent features

smartphone.fcm.mat <- melt(
  as.matrix(
    smartphone.fcm[smartphone.feat$feature, smartphone.feat$feature]),
  varnames = c("Var1", "Var2")) 
ggplot(data = smartphone.fcm.mat, 
       mapping = aes(x = Var1, y = Var2, fill = value)) +
  scale_fill_gradient2(
    low = "blue",
    high = "red",
    mid = "white",
    midpoint = 140,
    limit = c(0, 280),
    name = "Co-occurrence") +
  geom_tile() + 
  theme(axis.text.x = element_text(angle = 45, hjust = 1, size = 5),
        axis.text.y = element_text(size = 5)) +
  xlab("") +
  ylab("")

```

```{r}
# Produce clustering analysis. The co-occurrences are turned into dissimilarities before

smartphone.inv_occ <- 
  280 - as.matrix(
    smartphone.fcm[smartphone.feat$feature, smartphone.feat$feature]) ## 280 is the max co-occurrence here

smartphone.occ.hc <- hclust(as.dist(smartphone.inv_occ))
plot(smartphone.occ.hc)
```


### **Topic Modeling**

#### **LSA on TF (DFM)**

```{r, warning=FALSE}
Smartphone_reviews.lsa <- textmodel_lsa(
  x = Smartphone_reviews.dfm.group,
  nd = 10) 

head(Smartphone_reviews.lsa$docs)
head(Smartphone_reviews.lsa$features)

n.terms <- 5

## For Dimension 2
w.order <- sort(Smartphone_reviews.lsa$features[, 2], decreasing = TRUE)
w.top2 <- c(w.order[1:n.terms], rev(rev(w.order)[1:n.terms]))
## For Dimension 3
w.order <- sort(Smartphone_reviews.lsa$features[,3], decreasing = TRUE)
w.top3 <- c(w.order[1:n.terms], rev(rev(w.order)[1:n.terms]))

w.top2
w.top3

w.subset <- Smartphone_reviews.lsa$features[
    c(unique(c(names(w.top2), names(w.top3)))), 2:3]

biplot(
  y = Smartphone_reviews.lsa$docs[, 2:3],
  x = w.subset,
  col = c("black","red"),
  cex = c(0.8, 0.8),
  xlab = "Dim 2",
  ylab = "Dim 3")
```

#### **LSA on TF (TF-IDF)**

```{r, warning=FALSE}
Smartphone_reviews.lsa2 <- textmodel_lsa(Smartphone_reviews.tfidf.group, 
                                         nd = 10) 
head(Smartphone_reviews.lsa2$docs)
head(Smartphone_reviews.lsa2$features)
Smartphone_reviews.lsa2$sk

n.terms <- 5
w.order <- sort(Smartphone_reviews.lsa2$features[,2], decreasing = TRUE)
w.top2 <- c(w.order[1:n.terms], rev(rev(w.order)[1:n.terms]))

w.order <- sort(Smartphone_reviews.lsa2$features[,3], decreasing = TRUE)
w.top3 <- c(w.order[1:n.terms], rev(rev(w.order)[1:n.terms]))

w.subset <- Smartphone_reviews.lsa2$features[c(unique(c(names(w.top2), names(w.top3)))),2:3]
biplot(
  y = Smartphone_reviews.lsa2$docs[,2:3],
  x = w.subset,
  col = c("black", "red"),
  cex = c(0.8, 0.8),
  xlab = "Dim 2",
  ylab="Dim 3")
```


#### **LDA **

```{r}
# Term-Topic Analysis
set.seed(123)
Smartphone_reviews.lda <- textmodel_lda(x = Smartphone_reviews.dfm.group, k = 5)
seededlda::terms(Smartphone_reviews.lda, 5)
seededlda::topics(Smartphone_reviews.lda)
seededlda::topics(Smartphone_reviews.lda) %>% table()

# Term-Topic Analysis
phi.long <- melt(
  Smartphone_reviews.lda$phi,
  varnames = c("Topic", "Term"),
  value.name = "Phi") 

p1 <- phi.long %>% 
  group_by(Topic) %>% 
  top_n(10, Phi) %>% 
  ggplot(aes(reorder_within(Term, Phi, Topic), Phi, fill=Topic)) + 
  geom_col(show.legend = FALSE) +
  coord_flip() +
  facet_wrap(~ Topic, scales = "free_y") +
  scale_x_reordered() + 
  xlab("Term") + 
  theme(
    axis.text.y = element_text(size = 8),
    strip.text = element_text(size = 8))

# Topic-Document Analysis

set.seed(123)
theta.long <- melt(
  Smartphone_reviews.lda$theta,
  varnames = c("Doc", "Topic"),
  value.name = "Theta")

p2 <- theta.long %>% 
  group_by(Topic) %>% 
  top_n(10, Theta) %>% 
  ggplot(aes(reorder_within(Doc, Theta, Topic), Theta, fill=Topic)) + 
  geom_col(show.legend = FALSE) +
  coord_flip()+
  facet_wrap(~ Topic, scales = "free_y") +
  scale_x_reordered() + 
  xlab("Document") + 
  theme(
    axis.text.y = element_text(size = 8),
    strip.text = element_text(size = 8))

p1/p2



ggplot(theta.long,
       aes(x = Theta,
           y = Doc,
           fill = Topic)) + 
  geom_bar(stat="identity") +
  ylab("Model") 
```


##### **LDA Diagnostics across Topics - Prevalent, Coherence and Exclusivity -**

```{r}
# Most Prevalent Topic
rev(sort(colSums(Smartphone_reviews.lda$theta)/sum(Smartphone_reviews.lda$theta)))

# Topic Coherence
speech.codo <- fcm(
  Smartphone_reviews.dfm.group, 
  context = "document",
  count = "boolean",
  tri = FALSE) # co-document frequencies
term.mat <- seededlda::terms(Smartphone_reviews.lda, 5)
Coh <- rep(0, 5)
names(Coh) <- paste0("Topic", 1:5)
for (k in 1:5) {
  D.mat <- t(speech.codo[term.mat[,k], term.mat[,k]])
  D.vec <- Smartphone_reviews.dfm.group %>% 
    textstat_frequency %>% 
    filter(feature %in% term.mat[, k]) %>% 
    data.frame %>%
    select(feature, docfreq)
  for (m in 2:5){
    for (l in 1:(m - 1)) {
      vm <- term.mat[m, k]
      vl <- term.mat[l, k]
      Coh[k] <- Coh[k] + log((D.mat[vm, vl] + 1) / filter(D.vec, feature == vl)$docfreq)
    }
  }
}
rev(sort(Coh))

# See coherent Topic
as.matrix(speech.codo[term.mat[, 2], term.mat[, 2]])

# See least coherent Topic
as.matrix(speech.codo[term.mat[, 1], term.mat[, 1]])


# Exclusivity of the topic
excl <- rep(0, 5)
names(excl) <- paste0("Topic", 1:5)
for (k in 1:5) {
  for (i in 1:length(term.mat[,k])) {
    term.phi <- filter(phi.long, Term == term.mat[i,k])
    excl[k] <- excl[k] + filter(term.phi, Topic == "topic1")$Phi / sum(term.phi$Phi)
  }
  excl[k] <- excl[k] / length(term.mat[, k])
}
rev(sort(excl))
```



